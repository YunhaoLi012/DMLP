INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 5
INFO:train.train_function:  Instantaneous batch size per GPU = 32
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 32
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 69885
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 5
INFO:train.train_function:  Instantaneous batch size per GPU = 32
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 32
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 69885
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 5
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 139770
INFO:train.train_function:iter: 99; lr_train: 8.981080374384407e-05; loss: 2.300; loss_rec_train: 67.319; loss_kl_train: 32993.90625; ddpm: 1.199; 
INFO:train.train_function:iter: 199; lr_train: 8.961996850612598e-05; loss: 2.083; loss_rec_train: 61.881; loss_kl_train: 32496.716796875; ddpm: 0.938; 
INFO:train.train_function:iter: 299; lr_train: 8.942940653229959e-05; loss: 1.898; loss_rec_train: 57.649; loss_kl_train: 30712.83984375; ddpm: 0.802; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 5
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 5
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 5
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 139770
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 2795400
INFO:train.train_function:iter: 2767446; lr_train: 8.999053381883599e-05; loss: 1.878; loss_rec_train: 57.304; loss_kl_train: 29714.8359375; ddpm: 1.005; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 10000
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 279540000
INFO:train.train_function:iter: 2767446; lr_train: 8.999990533486935e-05; loss: 1.903; loss_rec_train: 54.556; loss_kl_train: 23296.69140625; ddpm: 0.991; 
INFO:train.train_function:iter: 5562846; lr_train: 8.999980971359328e-05; loss: 1.705; loss_rec_train: 50.769; loss_kl_train: 23602.763671875; ddpm: 0.838; 
INFO:train.train_function:iter: 8358246; lr_train: 8.999971409238568e-05; loss: 1.533; loss_rec_train: 51.733; loss_kl_train: 22493.0234375; ddpm: 0.748; 
INFO:train.train_function:iter: 11153646; lr_train: 8.999961847124644e-05; loss: 1.354; loss_rec_train: 46.538; loss_kl_train: 21054.5234375; ddpm: 0.604; 
INFO:train.train_function:iter: 13949046; lr_train: 8.999952285017566e-05; loss: 1.130; loss_rec_train: 44.036; loss_kl_train: 19709.79296875; ddpm: 0.450; 
INFO:train.train_function:iter: 16744446; lr_train: 8.999942722917325e-05; loss: 0.948; loss_rec_train: 43.278; loss_kl_train: 17873.30078125; ddpm: 0.258; 
INFO:train.train_function:iter: 19539846; lr_train: 8.999933160823928e-05; loss: 0.828; loss_rec_train: 39.888; loss_kl_train: 16695.09375; ddpm: 0.187; 
INFO:train.train_function:iter: 22335246; lr_train: 8.999923598737372e-05; loss: 0.779; loss_rec_train: 41.323; loss_kl_train: 15781.1796875; ddpm: 0.095; 
INFO:train.train_function:iter: 25130646; lr_train: 8.999914036657657e-05; loss: 0.685; loss_rec_train: 37.616; loss_kl_train: 15109.890625; ddpm: 0.081; 
INFO:train.train_function:iter: 27926046; lr_train: 8.999904474584783e-05; loss: 0.666; loss_rec_train: 37.198; loss_kl_train: 14601.1416015625; ddpm: 0.124; 
INFO:train.train_function:iter: 30721446; lr_train: 8.999894912518749e-05; loss: 0.660; loss_rec_train: 36.519; loss_kl_train: 14251.4130859375; ddpm: 0.054; 
INFO:train.train_function:iter: 33516846; lr_train: 8.99988535045956e-05; loss: 0.689; loss_rec_train: 38.899; loss_kl_train: 13927.283203125; ddpm: 0.089; 
INFO:train.train_function:iter: 36312246; lr_train: 8.999875788407208e-05; loss: 0.648; loss_rec_train: 36.096; loss_kl_train: 13661.23046875; ddpm: 0.044; 
INFO:train.train_function:iter: 39107646; lr_train: 8.999866226361702e-05; loss: 0.632; loss_rec_train: 36.889; loss_kl_train: 13464.732421875; ddpm: 0.071; 
INFO:train.train_function:iter: 41903046; lr_train: 8.999856664323033e-05; loss: 0.661; loss_rec_train: 38.123; loss_kl_train: 13220.220703125; ddpm: 0.048; 
INFO:train.train_function:iter: 44698446; lr_train: 8.999847102291208e-05; loss: 0.688; loss_rec_train: 39.820; loss_kl_train: 13019.607421875; ddpm: 0.031; 
INFO:train.train_function:iter: 47493846; lr_train: 8.999837540266223e-05; loss: 0.566; loss_rec_train: 34.007; loss_kl_train: 12810.212890625; ddpm: 0.050; 
INFO:train.train_function:iter: 50289246; lr_train: 8.999827978248081e-05; loss: 0.602; loss_rec_train: 35.730; loss_kl_train: 12626.73046875; ddpm: 0.048; 
INFO:train.train_function:iter: 53084646; lr_train: 8.999818416236778e-05; loss: 0.614; loss_rec_train: 35.806; loss_kl_train: 12463.05859375; ddpm: 0.054; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 10000
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 279540000
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 10000
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 279540000
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 10000
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 4472590000
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 2795400
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 2795400
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 10
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 279540
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 8
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 8
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 5590800
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:iter: 44278641; lr_train: 8.999940833741915e-05; loss: 0.003; loss_rec_train: 0.007; loss_kl_train: 246.40029907226562; ddpm: 0.000; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 1
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 1
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 44725900
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 2795400
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 100
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 2795400
INFO:train.train_function:iter: 99; lr_train: 8.999053381883599e-05; loss: 0.018; loss_rec_train: 1.658; loss_kl_train: 247.15505981445312; ddpm: 0.000; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:iter: 99; lr_train: 8.968482181692935e-05; loss: 0.042; loss_rec_train: 0.520; loss_kl_train: 247.31808471679688; ddpm: 0.004; 
INFO:train.train_function:iter: 199; lr_train: 8.936721546786228e-05; loss: 0.041; loss_rec_train: 0.633; loss_kl_train: 247.28102111816406; ddpm: 0.004; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:iter: 99; lr_train: 8.968482181692935e-05; loss: 0.024; loss_rec_train: 1.287; loss_kl_train: 247.2042999267578; ddpm: 0.002; 
INFO:train.train_function:iter: 199; lr_train: 8.936721546786228e-05; loss: 0.036; loss_rec_train: 1.880; loss_kl_train: 247.09854125976562; ddpm: 0.002; 
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 16
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 83862
INFO:train.train_function:***** Running training *****
INFO:train.train_function:  Num Epochs = 3
INFO:train.train_function:  Instantaneous batch size per GPU = 8
INFO:train.train_function:  Total train batch size (w. parallel, distributed & accumulation) = 8
INFO:train.train_function:  Gradient Accumulation steps = 1
INFO:train.train_function:  Total optimization steps = 167724
INFO:train.train_function:iter: 99; lr_train: 8.984231783572344e-05; loss: 0.013; loss_rec_train: 1.165; loss_kl_train: 247.26605224609375; ddpm: 0.000; 
INFO:train.train_function:iter: 199; lr_train: 8.96832318973018e-05; loss: 0.038; loss_rec_train: 1.779; loss_kl_train: 247.54244995117188; ddpm: 0.003; 
INFO:train.train_function:iter: 299; lr_train: 8.952433577056639e-05; loss: 0.022; loss_rec_train: 2.194; loss_kl_train: 247.54933166503906; ddpm: 0.000; 
INFO:train.train_function:iter: 399; lr_train: 8.936562934221371e-05; loss: 0.106; loss_rec_train: 2.799; loss_kl_train: 247.5915069580078; ddpm: 0.009; 
INFO:train.train_function:iter: 499; lr_train: 8.920711249894033e-05; loss: 0.049; loss_rec_train: 0.599; loss_kl_train: 246.87039184570312; ddpm: 0.004; 
INFO:train.train_function:iter: 599; lr_train: 8.904878512744267e-05; loss: 0.047; loss_rec_train: 3.694; loss_kl_train: 247.25009155273438; ddpm: 0.002; 
INFO:train.train_function:iter: 699; lr_train: 8.889064711441727e-05; loss: 0.015; loss_rec_train: 1.521; loss_kl_train: 247.24282836914062; ddpm: 0.000; 
INFO:train.train_function:iter: 799; lr_train: 8.873269834656061e-05; loss: 0.180; loss_rec_train: 0.586; loss_kl_train: 247.11788940429688; ddpm: 0.018; 
INFO:train.train_function:iter: 899; lr_train: 8.857493871056925e-05; loss: 0.054; loss_rec_train: 1.776; loss_kl_train: 247.4284210205078; ddpm: 0.004; 
INFO:train.train_function:iter: 999; lr_train: 8.841736809313962e-05; loss: 0.051; loss_rec_train: 3.991; loss_kl_train: 247.3837890625; ddpm: 0.002; 
INFO:train.train_function:iter: 1099; lr_train: 8.825998638096825e-05; loss: 0.025; loss_rec_train: 0.735; loss_kl_train: 246.8907928466797; ddpm: 0.002; 
INFO:train.train_function:iter: 1199; lr_train: 8.810279346075163e-05; loss: 0.024; loss_rec_train: 3.175; loss_kl_train: 247.51199340820312; ddpm: 0.000; 
INFO:train.train_function:iter: 1299; lr_train: 8.79457892191863e-05; loss: 0.091; loss_rec_train: 2.251; loss_kl_train: 247.174560546875; ddpm: 0.007; 
INFO:train.train_function:iter: 1399; lr_train: 8.778897354296871e-05; loss: 0.014; loss_rec_train: 1.122; loss_kl_train: 247.70619201660156; ddpm: 0.000; 
INFO:train.train_function:iter: 1499; lr_train: 8.763234631879538e-05; loss: 0.044; loss_rec_train: 0.968; loss_kl_train: 247.14913940429688; ddpm: 0.004; 
INFO:train.train_function:iter: 1599; lr_train: 8.747590743336287e-05; loss: 0.052; loss_rec_train: 4.529; loss_kl_train: 248.04029846191406; ddpm: 0.002; 
INFO:train.train_function:iter: 1699; lr_train: 8.731965677336758e-05; loss: 0.046; loss_rec_train: 0.445; loss_kl_train: 246.99227905273438; ddpm: 0.004; 
INFO:train.train_function:iter: 1799; lr_train: 8.716359422550605e-05; loss: 0.053; loss_rec_train: 2.886; loss_kl_train: 247.2880859375; ddpm: 0.003; 
INFO:train.train_function:iter: 1899; lr_train: 8.700771967647479e-05; loss: 0.018; loss_rec_train: 1.320; loss_kl_train: 247.4864044189453; ddpm: 0.000; 
INFO:train.train_function:iter: 1999; lr_train: 8.685203301297034e-05; loss: 0.033; loss_rec_train: 2.390; loss_kl_train: 247.63162231445312; ddpm: 0.002; 
INFO:train.train_function:iter: 2099; lr_train: 8.669653412168912e-05; loss: 0.006; loss_rec_train: 0.074; loss_kl_train: 246.69610595703125; ddpm: 0.001; 
INFO:train.train_function:iter: 2199; lr_train: 8.654122288932769e-05; loss: 0.070; loss_rec_train: 2.895; loss_kl_train: 247.73941040039062; ddpm: 0.004; 
INFO:train.train_function:iter: 2299; lr_train: 8.638609920258256e-05; loss: 0.009; loss_rec_train: 0.725; loss_kl_train: 247.44944763183594; ddpm: 0.000; 
INFO:train.train_function:iter: 2399; lr_train: 8.623116294815017e-05; loss: 0.032; loss_rec_train: 1.213; loss_kl_train: 247.28280639648438; ddpm: 0.002; 
INFO:train.train_function:iter: 2499; lr_train: 8.607641401272706e-05; loss: 0.025; loss_rec_train: 1.166; loss_kl_train: 247.4894561767578; ddpm: 0.002; 
INFO:train.train_function:iter: 2599; lr_train: 8.592185228300973e-05; loss: 0.320; loss_rec_train: 2.431; loss_kl_train: 247.40121459960938; ddpm: 0.030; 
INFO:train.train_function:iter: 2699; lr_train: 8.57674776456947e-05; loss: 0.052; loss_rec_train: 1.726; loss_kl_train: 247.95664978027344; ddpm: 0.004; 
INFO:train.train_function:iter: 2799; lr_train: 8.561328998747844e-05; loss: 0.059; loss_rec_train: 2.933; loss_kl_train: 247.258544921875; ddpm: 0.004; 
INFO:train.train_function:iter: 2899; lr_train: 8.545928919505745e-05; loss: 0.429; loss_rec_train: 1.838; loss_kl_train: 247.00064086914062; ddpm: 0.041; 
INFO:train.train_function:iter: 2999; lr_train: 8.530547515512828e-05; loss: 0.024; loss_rec_train: 2.641; loss_kl_train: 247.64767456054688; ddpm: 0.001; 
INFO:train.train_function:iter: 3099; lr_train: 8.515184775438736e-05; loss: 0.224; loss_rec_train: 2.440; loss_kl_train: 247.5694122314453; ddpm: 0.021; 
INFO:train.train_function:iter: 3199; lr_train: 8.499840687953122e-05; loss: 0.350; loss_rec_train: 1.619; loss_kl_train: 247.16567993164062; ddpm: 0.034; 
INFO:train.train_function:iter: 3299; lr_train: 8.484515241725639e-05; loss: 0.018; loss_rec_train: 0.785; loss_kl_train: 247.1451416015625; ddpm: 0.001; 
INFO:train.train_function:iter: 3399; lr_train: 8.469208425425935e-05; loss: 0.129; loss_rec_train: 2.796; loss_kl_train: 247.40570068359375; ddpm: 0.011; 
INFO:train.train_function:iter: 3499; lr_train: 8.453920227723658e-05; loss: 0.016; loss_rec_train: 1.257; loss_kl_train: 247.049560546875; ddpm: 0.001; 
INFO:train.train_function:iter: 3599; lr_train: 8.438650637288462e-05; loss: 0.022; loss_rec_train: 2.409; loss_kl_train: 247.58279418945312; ddpm: 0.000; 
INFO:train.train_function:iter: 3699; lr_train: 8.423399642789993e-05; loss: 0.275; loss_rec_train: 5.212; loss_kl_train: 247.89453125; ddpm: 0.024; 
INFO:train.train_function:iter: 3799; lr_train: 8.408167232897907e-05; loss: 0.012; loss_rec_train: 1.152; loss_kl_train: 247.17343139648438; ddpm: 0.000; 
INFO:train.train_function:iter: 3899; lr_train: 8.392953396281848e-05; loss: 0.043; loss_rec_train: 0.968; loss_kl_train: 247.27450561523438; ddpm: 0.003; 
INFO:train.train_function:iter: 3999; lr_train: 8.377758121611467e-05; loss: 0.075; loss_rec_train: 5.733; loss_kl_train: 247.54534912109375; ddpm: 0.004; 
INFO:train.train_function:iter: 4099; lr_train: 8.362581397556421e-05; loss: 0.036; loss_rec_train: 3.787; loss_kl_train: 247.37107849121094; ddpm: 0.001; 
INFO:train.train_function:iter: 4199; lr_train: 8.34742321278635e-05; loss: 0.067; loss_rec_train: 1.530; loss_kl_train: 247.06565856933594; ddpm: 0.005; 
INFO:train.train_function:iter: 4299; lr_train: 8.332283555970911e-05; loss: 0.041; loss_rec_train: 2.566; loss_kl_train: 247.95396423339844; ddpm: 0.002; 
INFO:train.train_function:iter: 4399; lr_train: 8.31716241577975e-05; loss: 0.023; loss_rec_train: 1.212; loss_kl_train: 247.0973663330078; ddpm: 0.001; 
INFO:train.train_function:iter: 4499; lr_train: 8.302059780882523e-05; loss: 0.050; loss_rec_train: 1.729; loss_kl_train: 247.3870849609375; ddpm: 0.004; 
INFO:train.train_function:iter: 4599; lr_train: 8.286975639948875e-05; loss: 0.052; loss_rec_train: 3.889; loss_kl_train: 247.82467651367188; ddpm: 0.003; 
INFO:train.train_function:iter: 4699; lr_train: 8.271909981648455e-05; loss: 0.012; loss_rec_train: 0.929; loss_kl_train: 247.08767700195312; ddpm: 0.000; 
INFO:train.train_function:iter: 4799; lr_train: 8.256862794650918e-05; loss: 0.021; loss_rec_train: 2.503; loss_kl_train: 247.83602905273438; ddpm: 0.000; 
INFO:train.train_function:iter: 4899; lr_train: 8.241834067625913e-05; loss: 0.022; loss_rec_train: 2.415; loss_kl_train: 247.35752868652344; ddpm: 0.000; 
INFO:train.train_function:iter: 4999; lr_train: 8.226823789243087e-05; loss: 0.040; loss_rec_train: 4.248; loss_kl_train: 247.55267333984375; ddpm: 0.001; 
INFO:train.train_function:iter: 5099; lr_train: 8.21183194817209e-05; loss: 0.018; loss_rec_train: 1.746; loss_kl_train: 247.33766174316406; ddpm: 0.000; 
INFO:train.train_function:iter: 5199; lr_train: 8.19685853308258e-05; loss: 0.064; loss_rec_train: 3.025; loss_kl_train: 247.29971313476562; ddpm: 0.004; 
INFO:train.train_function:iter: 5299; lr_train: 8.181903532644196e-05; loss: 0.031; loss_rec_train: 1.600; loss_kl_train: 247.07382202148438; ddpm: 0.002; 
INFO:train.train_function:iter: 5399; lr_train: 8.166966935526595e-05; loss: 0.089; loss_rec_train: 1.339; loss_kl_train: 247.23733520507812; ddpm: 0.008; 
INFO:train.train_function:iter: 5499; lr_train: 8.152048730399425e-05; loss: 0.122; loss_rec_train: 3.423; loss_kl_train: 247.7117919921875; ddpm: 0.010; 
INFO:train.train_function:iter: 5599; lr_train: 8.13714890593234e-05; loss: 0.055; loss_rec_train: 1.251; loss_kl_train: 247.36524963378906; ddpm: 0.005; 
INFO:train.train_function:iter: 5699; lr_train: 8.122267450794982e-05; loss: 0.704; loss_rec_train: 3.703; loss_kl_train: 247.1633758544922; ddpm: 0.068; 
INFO:train.train_function:iter: 5799; lr_train: 8.107404353657009e-05; loss: 0.028; loss_rec_train: 3.249; loss_kl_train: 247.1043701171875; ddpm: 0.000; 
INFO:train.train_function:iter: 5899; lr_train: 8.092559603188069e-05; loss: 0.072; loss_rec_train: 4.850; loss_kl_train: 247.93325805664062; ddpm: 0.004; 
INFO:train.train_function:iter: 5999; lr_train: 8.077733188057808e-05; loss: 0.038; loss_rec_train: 3.610; loss_kl_train: 247.38340759277344; ddpm: 0.001; 
INFO:train.train_function:iter: 6099; lr_train: 8.06292509693588e-05; loss: 0.027; loss_rec_train: 1.309; loss_kl_train: 247.43136596679688; ddpm: 0.002; 
INFO:train.train_function:iter: 6199; lr_train: 8.048135318491934e-05; loss: 0.179; loss_rec_train: 2.359; loss_kl_train: 247.6830596923828; ddpm: 0.016; 
INFO:train.train_function:iter: 6299; lr_train: 8.033363841395623e-05; loss: 0.053; loss_rec_train: 5.232; loss_kl_train: 247.45669555664062; ddpm: 0.001; 
INFO:train.train_function:iter: 6399; lr_train: 8.018610654316593e-05; loss: 0.052; loss_rec_train: 5.985; loss_kl_train: 247.41717529296875; ddpm: 0.001; 
INFO:train.train_function:iter: 6499; lr_train: 8.003875745924495e-05; loss: 0.121; loss_rec_train: 3.845; loss_kl_train: 247.6702117919922; ddpm: 0.009; 
INFO:train.train_function:iter: 6599; lr_train: 7.989159104888983e-05; loss: 0.040; loss_rec_train: 2.771; loss_kl_train: 247.38656616210938; ddpm: 0.002; 
INFO:train.train_function:iter: 6699; lr_train: 7.974460719879702e-05; loss: 0.035; loss_rec_train: 2.882; loss_kl_train: 247.46826171875; ddpm: 0.001; 
INFO:train.train_function:iter: 6799; lr_train: 7.959780579566303e-05; loss: 0.034; loss_rec_train: 3.180; loss_kl_train: 247.29296875; ddpm: 0.000; 
INFO:train.train_function:iter: 6899; lr_train: 7.945118672618438e-05; loss: 0.081; loss_rec_train: 4.837; loss_kl_train: 247.41482543945312; ddpm: 0.004; 
INFO:train.train_function:iter: 6999; lr_train: 7.930474987705758e-05; loss: 0.026; loss_rec_train: 2.690; loss_kl_train: 247.50180053710938; ddpm: 0.000; 
INFO:train.train_function:iter: 7099; lr_train: 7.915849513497911e-05; loss: 0.092; loss_rec_train: 3.504; loss_kl_train: 247.68101501464844; ddpm: 0.007; 
INFO:train.train_function:iter: 7199; lr_train: 7.901242238664546e-05; loss: 0.048; loss_rec_train: 1.409; loss_kl_train: 247.2693328857422; ddpm: 0.004; 
INFO:train.train_function:iter: 7299; lr_train: 7.886653151875315e-05; loss: 0.077; loss_rec_train: 3.447; loss_kl_train: 247.90890502929688; ddpm: 0.005; 
INFO:train.train_function:iter: 7399; lr_train: 7.872082241799871e-05; loss: 0.049; loss_rec_train: 2.544; loss_kl_train: 247.34425354003906; ddpm: 0.003; 
INFO:train.train_function:iter: 7499; lr_train: 7.857529497107858e-05; loss: 0.036; loss_rec_train: 2.362; loss_kl_train: 247.53323364257812; ddpm: 0.002; 
INFO:train.train_function:iter: 7599; lr_train: 7.84299490646893e-05; loss: 0.156; loss_rec_train: 0.814; loss_kl_train: 246.92178344726562; ddpm: 0.015; 
INFO:train.train_function:iter: 7699; lr_train: 7.828478458552738e-05; loss: 0.192; loss_rec_train: 1.833; loss_kl_train: 247.432373046875; ddpm: 0.018; 
INFO:train.train_function:iter: 7799; lr_train: 7.813980142028928e-05; loss: 0.087; loss_rec_train: 3.851; loss_kl_train: 247.6221160888672; ddpm: 0.006; 
INFO:train.train_function:iter: 7899; lr_train: 7.799499945567153e-05; loss: 0.038; loss_rec_train: 3.581; loss_kl_train: 247.37271118164062; ddpm: 0.000; 
INFO:train.train_function:iter: 7999; lr_train: 7.785037857837063e-05; loss: 1.281; loss_rec_train: 5.663; loss_kl_train: 247.57623291015625; ddpm: 0.125; 
INFO:train.train_function:iter: 8099; lr_train: 7.77059386750831e-05; loss: 0.234; loss_rec_train: 1.809; loss_kl_train: 247.05862426757812; ddpm: 0.022; 
INFO:train.train_function:iter: 8199; lr_train: 7.75616796325054e-05; loss: 0.013; loss_rec_train: 0.367; loss_kl_train: 247.23101806640625; ddpm: 0.001; 
INFO:train.train_function:iter: 8299; lr_train: 7.741760133733403e-05; loss: 0.025; loss_rec_train: 2.254; loss_kl_train: 247.18157958984375; ddpm: 0.001; 
INFO:train.train_function:iter: 8399; lr_train: 7.727370367626554e-05; loss: 0.066; loss_rec_train: 2.709; loss_kl_train: 247.39541625976562; ddpm: 0.005; 
INFO:train.train_function:iter: 8499; lr_train: 7.712998653599641e-05; loss: 0.040; loss_rec_train: 2.451; loss_kl_train: 247.8592071533203; ddpm: 0.002; 
INFO:train.train_function:iter: 8599; lr_train: 7.698644980322311e-05; loss: 0.026; loss_rec_train: 2.516; loss_kl_train: 247.50767517089844; ddpm: 0.001; 
INFO:train.train_function:iter: 8699; lr_train: 7.684309336464217e-05; loss: 0.042; loss_rec_train: 6.257; loss_kl_train: 247.3348388671875; ddpm: 0.000; 
INFO:train.train_function:iter: 8799; lr_train: 7.669991710695012e-05; loss: 0.031; loss_rec_train: 2.865; loss_kl_train: 247.29034423828125; ddpm: 0.000; 
INFO:train.train_function:iter: 8899; lr_train: 7.65569209168434e-05; loss: 0.010; loss_rec_train: 0.674; loss_kl_train: 247.08724975585938; ddpm: 0.000; 
INFO:train.train_function:iter: 8999; lr_train: 7.641410468101852e-05; loss: 0.021; loss_rec_train: 2.360; loss_kl_train: 247.4606475830078; ddpm: 0.001; 
INFO:train.train_function:iter: 9099; lr_train: 7.627146828617204e-05; loss: 0.034; loss_rec_train: 1.090; loss_kl_train: 247.0288848876953; ddpm: 0.002; 
INFO:train.train_function:iter: 9199; lr_train: 7.612901161900041e-05; loss: 0.036; loss_rec_train: 2.648; loss_kl_train: 246.93701171875; ddpm: 0.001; 
INFO:train.train_function:iter: 9299; lr_train: 7.598673456620014e-05; loss: 0.644; loss_rec_train: 3.049; loss_kl_train: 247.45492553710938; ddpm: 0.062; 
INFO:train.train_function:iter: 9399; lr_train: 7.584463701446772e-05; loss: 0.030; loss_rec_train: 2.056; loss_kl_train: 246.7894287109375; ddpm: 0.001; 
INFO:train.train_function:iter: 9499; lr_train: 7.57027188504997e-05; loss: 0.138; loss_rec_train: 2.762; loss_kl_train: 247.8046875; ddpm: 0.012; 
INFO:train.train_function:iter: 9599; lr_train: 7.556097996099252e-05; loss: 0.029; loss_rec_train: 3.633; loss_kl_train: 247.39089965820312; ddpm: 0.000; 
INFO:train.train_function:iter: 9699; lr_train: 7.541942023264271e-05; loss: 0.027; loss_rec_train: 2.997; loss_kl_train: 247.46609497070312; ddpm: 0.000; 
INFO:train.train_function:iter: 9799; lr_train: 7.527803955214677e-05; loss: 0.087; loss_rec_train: 2.858; loss_kl_train: 247.71121215820312; ddpm: 0.007; 
INFO:train.train_function:iter: 9899; lr_train: 7.513683780620123e-05; loss: 0.018; loss_rec_train: 1.498; loss_kl_train: 247.61740112304688; ddpm: 0.000; 
INFO:train.train_function:iter: 9999; lr_train: 7.499581488150254e-05; loss: 0.067; loss_rec_train: 2.235; loss_kl_train: 247.64291381835938; ddpm: 0.005; 
INFO:train.train_function:iter: 10099; lr_train: 7.485497066474721e-05; loss: 0.104; loss_rec_train: 3.085; loss_kl_train: 246.94113159179688; ddpm: 0.008; 
INFO:train.train_function:iter: 10199; lr_train: 7.47143050426318e-05; loss: 0.046; loss_rec_train: 1.191; loss_kl_train: 246.98233032226562; ddpm: 0.003; 
INFO:train.train_function:iter: 10299; lr_train: 7.457381790185271e-05; loss: 0.038; loss_rec_train: 1.362; loss_kl_train: 247.56411743164062; ddpm: 0.003; 
INFO:train.train_function:iter: 10399; lr_train: 7.443350912910652e-05; loss: 0.132; loss_rec_train: 2.123; loss_kl_train: 247.26820373535156; ddpm: 0.011; 
INFO:train.train_function:iter: 10499; lr_train: 7.42933786110897e-05; loss: 0.089; loss_rec_train: 5.135; loss_kl_train: 247.7178955078125; ddpm: 0.005; 
INFO:train.train_function:iter: 10599; lr_train: 7.415342623449879e-05; loss: 0.047; loss_rec_train: 3.329; loss_kl_train: 247.3309326171875; ddpm: 0.002; 
INFO:train.train_function:iter: 10699; lr_train: 7.401365188603025e-05; loss: 0.024; loss_rec_train: 2.212; loss_kl_train: 247.0425262451172; ddpm: 0.001; 
INFO:train.train_function:iter: 10799; lr_train: 7.387405545238058e-05; loss: 0.150; loss_rec_train: 0.991; loss_kl_train: 247.07850646972656; ddpm: 0.014; 
INFO:train.train_function:iter: 10899; lr_train: 7.373463682024628e-05; loss: 0.161; loss_rec_train: 3.017; loss_kl_train: 247.59609985351562; ddpm: 0.014; 
INFO:train.train_function:iter: 10999; lr_train: 7.359539587632391e-05; loss: 0.040; loss_rec_train: 3.435; loss_kl_train: 247.46510314941406; ddpm: 0.002; 
INFO:train.train_function:iter: 11099; lr_train: 7.345633250730989e-05; loss: 0.108; loss_rec_train: 0.905; loss_kl_train: 247.31887817382812; ddpm: 0.010; 
INFO:train.train_function:iter: 11199; lr_train: 7.331744659990077e-05; loss: 0.185; loss_rec_train: 3.858; loss_kl_train: 247.60150146484375; ddpm: 0.016; 
INFO:train.train_function:iter: 11299; lr_train: 7.317873804079305e-05; loss: 0.062; loss_rec_train: 5.799; loss_kl_train: 247.80364990234375; ddpm: 0.002; 
INFO:train.train_function:iter: 11399; lr_train: 7.30402067166832e-05; loss: 0.110; loss_rec_train: 3.987; loss_kl_train: 247.5712890625; ddpm: 0.008; 
INFO:train.train_function:iter: 11499; lr_train: 7.290185251426774e-05; loss: 0.029; loss_rec_train: 3.060; loss_kl_train: 247.85089111328125; ddpm: 0.000; 
INFO:train.train_function:iter: 11599; lr_train: 7.276367532024318e-05; loss: 0.699; loss_rec_train: 5.649; loss_kl_train: 247.3661651611328; ddpm: 0.067; 
INFO:train.train_function:iter: 11699; lr_train: 7.262567502130603e-05; loss: 0.073; loss_rec_train: 3.762; loss_kl_train: 247.60089111328125; ddpm: 0.004; 
INFO:train.train_function:iter: 11799; lr_train: 7.248785150415275e-05; loss: 0.058; loss_rec_train: 4.161; loss_kl_train: 247.32447814941406; ddpm: 0.003; 
INFO:train.train_function:iter: 11899; lr_train: 7.235020465547987e-05; loss: 0.033; loss_rec_train: 4.250; loss_kl_train: 247.5028076171875; ddpm: 0.000; 
INFO:train.train_function:iter: 11999; lr_train: 7.22127343619839e-05; loss: 0.035; loss_rec_train: 0.503; loss_kl_train: 247.05162048339844; ddpm: 0.003; 
INFO:train.train_function:iter: 12099; lr_train: 7.207544051036133e-05; loss: 0.077; loss_rec_train: 1.048; loss_kl_train: 247.38580322265625; ddpm: 0.007; 
INFO:train.train_function:iter: 12199; lr_train: 7.193832298730865e-05; loss: 0.011; loss_rec_train: 0.899; loss_kl_train: 247.13308715820312; ddpm: 0.000; 
INFO:train.train_function:iter: 12299; lr_train: 7.180138167952238e-05; loss: 0.015; loss_rec_train: 1.140; loss_kl_train: 247.10682678222656; ddpm: 0.000; 
INFO:train.train_function:iter: 12399; lr_train: 7.166461647369903e-05; loss: 0.011; loss_rec_train: 0.967; loss_kl_train: 247.75387573242188; ddpm: 0.000; 
INFO:train.train_function:iter: 12499; lr_train: 7.152802725653505e-05; loss: 0.044; loss_rec_train: 4.761; loss_kl_train: 247.53314208984375; ddpm: 0.001; 
INFO:train.train_function:iter: 12599; lr_train: 7.139161391472699e-05; loss: 0.309; loss_rec_train: 2.016; loss_kl_train: 247.4014892578125; ddpm: 0.030; 
INFO:train.train_function:iter: 12699; lr_train: 7.125537633497135e-05; loss: 0.016; loss_rec_train: 1.354; loss_kl_train: 247.12918090820312; ddpm: 0.001; 
INFO:train.train_function:iter: 12799; lr_train: 7.11193144039646e-05; loss: 0.028; loss_rec_train: 2.992; loss_kl_train: 247.18511962890625; ddpm: 0.000; 
INFO:train.train_function:iter: 12899; lr_train: 7.098342800840325e-05; loss: 0.261; loss_rec_train: 2.170; loss_kl_train: 246.93516540527344; ddpm: 0.025; 
INFO:train.train_function:iter: 12999; lr_train: 7.084771703498382e-05; loss: 0.062; loss_rec_train: 2.137; loss_kl_train: 247.6249542236328; ddpm: 0.005; 
INFO:train.train_function:iter: 13099; lr_train: 7.071218137040281e-05; loss: 0.031; loss_rec_train: 3.689; loss_kl_train: 247.26422119140625; ddpm: 0.000; 
INFO:train.train_function:iter: 13199; lr_train: 7.057682090135672e-05; loss: 0.035; loss_rec_train: 1.940; loss_kl_train: 247.54296875; ddpm: 0.002; 
INFO:train.train_function:iter: 13299; lr_train: 7.044163551454202e-05; loss: 0.072; loss_rec_train: 0.954; loss_kl_train: 246.97772216796875; ddpm: 0.006; 
INFO:train.train_function:iter: 13399; lr_train: 7.030662509665524e-05; loss: 0.375; loss_rec_train: 3.442; loss_kl_train: 247.63258361816406; ddpm: 0.036; 
INFO:train.train_function:iter: 13499; lr_train: 7.01717895343929e-05; loss: 0.196; loss_rec_train: 3.167; loss_kl_train: 247.44903564453125; ddpm: 0.017; 
INFO:train.train_function:iter: 13599; lr_train: 7.003712871445146e-05; loss: 0.032; loss_rec_train: 2.662; loss_kl_train: 247.16476440429688; ddpm: 0.000; 
INFO:train.train_function:iter: 13699; lr_train: 6.990264252352743e-05; loss: 0.127; loss_rec_train: 1.535; loss_kl_train: 247.22865295410156; ddpm: 0.012; 
INFO:train.train_function:iter: 13799; lr_train: 6.976833084831736e-05; loss: 0.013; loss_rec_train: 0.839; loss_kl_train: 247.0301513671875; ddpm: 0.001; 
INFO:train.train_function:iter: 13899; lr_train: 6.963419357551767e-05; loss: 0.012; loss_rec_train: 0.675; loss_kl_train: 247.40557861328125; ddpm: 0.001; 
INFO:train.train_function:iter: 13999; lr_train: 6.950023059182491e-05; loss: 0.012; loss_rec_train: 0.195; loss_kl_train: 247.08389282226562; ddpm: 0.001; 
INFO:train.train_function:iter: 14099; lr_train: 6.936644178393558e-05; loss: 0.029; loss_rec_train: 1.593; loss_kl_train: 247.33245849609375; ddpm: 0.002; 
INFO:train.train_function:iter: 14199; lr_train: 6.92328270385462e-05; loss: 0.024; loss_rec_train: 1.928; loss_kl_train: 247.2984619140625; ddpm: 0.001; 
INFO:train.train_function:iter: 14299; lr_train: 6.909938624235322e-05; loss: 0.088; loss_rec_train: 1.556; loss_kl_train: 247.02398681640625; ddpm: 0.007; 
INFO:train.train_function:iter: 14399; lr_train: 6.896611928205316e-05; loss: 0.038; loss_rec_train: 4.493; loss_kl_train: 247.60931396484375; ddpm: 0.001; 
INFO:train.train_function:iter: 14499; lr_train: 6.883302604434254e-05; loss: 0.042; loss_rec_train: 2.656; loss_kl_train: 247.50485229492188; ddpm: 0.002; 
INFO:train.train_function:iter: 14599; lr_train: 6.870010641591787e-05; loss: 0.015; loss_rec_train: 1.498; loss_kl_train: 247.35194396972656; ddpm: 0.000; 
INFO:train.train_function:iter: 14699; lr_train: 6.856736028347562e-05; loss: 0.423; loss_rec_train: 2.904; loss_kl_train: 247.2510528564453; ddpm: 0.040; 
INFO:train.train_function:iter: 14799; lr_train: 6.843478753371229e-05; loss: 0.088; loss_rec_train: 4.203; loss_kl_train: 247.57691955566406; ddpm: 0.005; 
INFO:train.train_function:iter: 14899; lr_train: 6.830238805332442e-05; loss: 0.053; loss_rec_train: 3.966; loss_kl_train: 247.50558471679688; ddpm: 0.002; 
INFO:train.train_function:iter: 14999; lr_train: 6.817016172900846e-05; loss: 0.025; loss_rec_train: 0.281; loss_kl_train: 247.06398010253906; ddpm: 0.002; 
INFO:train.train_function:iter: 15099; lr_train: 6.803810844746095e-05; loss: 0.033; loss_rec_train: 2.095; loss_kl_train: 247.00802612304688; ddpm: 0.001; 
INFO:train.train_function:iter: 15199; lr_train: 6.790622809537836e-05; loss: 0.006; loss_rec_train: 0.487; loss_kl_train: 246.74456787109375; ddpm: 0.000; 
INFO:train.train_function:iter: 15299; lr_train: 6.777452055945724e-05; loss: 0.063; loss_rec_train: 4.589; loss_kl_train: 247.9489288330078; ddpm: 0.003; 
INFO:train.train_function:iter: 15399; lr_train: 6.764298572639405e-05; loss: 0.600; loss_rec_train: 3.925; loss_kl_train: 247.62294006347656; ddpm: 0.057; 
INFO:train.train_function:iter: 15499; lr_train: 6.751162348288529e-05; loss: 0.134; loss_rec_train: 1.221; loss_kl_train: 247.34666442871094; ddpm: 0.012; 
INFO:train.train_function:iter: 15599; lr_train: 6.738043371562747e-05; loss: 0.073; loss_rec_train: 2.210; loss_kl_train: 247.32635498046875; ddpm: 0.006; 
INFO:train.train_function:iter: 15699; lr_train: 6.724941631131713e-05; loss: 0.027; loss_rec_train: 2.899; loss_kl_train: 247.49383544921875; ddpm: 0.001; 
INFO:train.train_function:iter: 15799; lr_train: 6.711857115665069e-05; loss: 0.121; loss_rec_train: 4.716; loss_kl_train: 247.8701629638672; ddpm: 0.009; 
INFO:train.train_function:iter: 15899; lr_train: 6.698789813832471e-05; loss: 0.221; loss_rec_train: 2.769; loss_kl_train: 247.68624877929688; ddpm: 0.020; 
INFO:train.train_function:iter: 15999; lr_train: 6.685739714303572e-05; loss: 0.432; loss_rec_train: 1.460; loss_kl_train: 247.1649169921875; ddpm: 0.042; 
INFO:train.train_function:iter: 16099; lr_train: 6.672706805748014e-05; loss: 0.036; loss_rec_train: 2.703; loss_kl_train: 247.55970764160156; ddpm: 0.002; 
INFO:train.train_function:iter: 16199; lr_train: 6.65969107683545e-05; loss: 0.059; loss_rec_train: 1.532; loss_kl_train: 247.32400512695312; ddpm: 0.005; 
INFO:train.train_function:iter: 16299; lr_train: 6.646692516235536e-05; loss: 0.011; loss_rec_train: 0.921; loss_kl_train: 246.9557342529297; ddpm: 0.000; 
INFO:train.train_function:iter: 16399; lr_train: 6.633711112617914e-05; loss: 0.053; loss_rec_train: 3.899; loss_kl_train: 247.2708740234375; ddpm: 0.001; 
