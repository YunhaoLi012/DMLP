{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jieqi/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset,concatenate_datasets, Dataset,DatasetDict\n",
    "# from transformers import AutoTokenizer\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.my_transformers import *\n",
    "# from models.models import VAE, DDPM, MLPSkipNet, TransformerNet, VAE_DDPM\n",
    "# from train.reconstruction import *\n",
    "# from functions import weights_init_rondom\n",
    "# from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyCollator(object):\n",
    "#     def __init__(self, encoder_token, decoder_token):\n",
    "#         self.encoder_token = encoder_token\n",
    "#         self.decoder_token = decoder_token\n",
    "#     def __call__(self, batch):\n",
    "#         input_ids_bert = pad_sequence([torch.tensor(f['bert_token'], dtype=torch.long) for f in batch],\n",
    "#                                   batch_first=True, padding_value=self.encoder_token)\n",
    "#         input_ids_gpt = pad_sequence([torch.tensor(f['gpt2_token'], dtype=torch.long) for f in batch],\n",
    "#                                     batch_first=True, padding_value=self.decoder_token)\n",
    "#         try:\n",
    "#             token_lengths = torch.tensor([[len(f['bert_token']), len(f['gpt2_token'])] for f in batch],\n",
    "#                                         dtype=torch.long)\n",
    "#         except:\n",
    "#             token_lengths = torch.zeros((len(batch), 1091))\n",
    "#             for i in range(len(batch)):\n",
    "#                 token_lengths[i, len(batch[i]['gpt2_token'])] = 1\n",
    "#         return (input_ids_bert, input_ids_gpt, token_lengths)\n",
    "    \n",
    "# encoder_model_class = MODEL_CLASS['BertForLatentConnectorAVG']\n",
    "# tokenizer_encoder = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "# latent_size = 64\n",
    "# model_encoder = encoder_model_class.from_pretrained(\"prajjwal1/bert-small\", latent_size=latent_size,\n",
    "#                                                     pad_id=tokenizer_encoder.pad_token_id,local_files_only=False)\n",
    "\n",
    "\n",
    "# decoder_model_class = MODEL_CLASS['GPT2ForLatentConnectorNew']\n",
    "# tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "# model_decoder = decoder_model_class.from_pretrained(\"gpt2-xl\", latent_size=latent_size,\n",
    "#                                                         latent_as_gpt_emb=True,\n",
    "#                                                         latent_as_gpt_memory=True,local_files_only=False)\n",
    "# decoder_n_layer = model_decoder.transformer.config.n_layer\n",
    "# model_decoder.transformer.change_order()\n",
    "\n",
    "# special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>', }\n",
    "# num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "# model_decoder.resize_token_embeddings(len(tokenizer_decoder))\n",
    "# bert_pad_token = tokenizer_encoder.pad_token_id\n",
    "# gpt2_pad_token = tokenizer_decoder.pad_token_id\n",
    "\n",
    "# my_collator = MyCollator(bert_pad_token, gpt2_pad_token)\n",
    "\n",
    "# train_eval_dataset =load_dataset(\"guangyil/yelp_short_v2\")\n",
    "# eval_dataloader =  DataLoader(train_eval_dataset['test'], num_workers=0, collate_fn=my_collator,batch_size=64)\n",
    "# train_dataloader = DataLoader(train_eval_dataset['train'], num_workers=0, collate_fn=my_collator, batch_size=64)\n",
    "\n",
    "# output_dir = \"test\"\n",
    "# model_vae = VAE(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, latent_size, output_dir)\n",
    "\n",
    "# model_vae.apply(weights_init_rondom)\n",
    "# model_vae.to('cuda')\n",
    "\n",
    "# calc_rec_lgy(model_vae, tokenizer_encoder, tokenizer_decoder,eval_dataloader, \"cuda\", True, ns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets, Dataset,DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "from models.my_transformers import *\n",
    "from models.models import VAE, DDPM, MLPSkipNet, TransformerNet,VAE_DDPM\n",
    "from train.reconstruction import *\n",
    "from functions import weights_init_rondom\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "initialize models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForLatentConnectorAVG were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['bert.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2ForLatentConnectorNew were not initialized from the model checkpoint at gpt2-xl and are newly initialized: ['h.48.attn.c_proj.bias', 'h.48.attn.c_attn.weight', 'lm_head.bias', 'h.48.ln_2.weight', 'h.48.attn.c_proj.weight', 'h.48.ln_1.bias', 'h.48.mlp.c_fc.bias', 'h.48.mlp.c_proj.bias', 'h.48.ln_1.weight', 'h.48.attn.c_attn.bias', 'linear_emb.weight', 'h.48.ln_2.bias', 'linear.weight', 'h.48.mlp.c_proj.weight', 'h.48.mlp.c_fc.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\n",
      "\n",
      "\n",
      "\n",
      "ÂÂÂ Dj Dj tack [* Windsor Windsor MV Bers Bers Bers Bers Bers Bers Bers Bers Bers Bers Bers Bers Sle Sle Sle Sle Sle Sle Sle Sle Pier Pier\n",
      "\n",
      "admission admission admission admission admission admission admission admission admission admission che admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission admission\n",
      "\n",
      "itaodanodanultureodanitaitaitaitaitaita sanctuaryitaitaitaitaitaitaitaitaitaitaitaitaitaitaitaitaitaitaitaita\n",
      "\n",
      "Nova Pr Pr Pr Pr Pr Pr anim Pr anim anim anim anim anim anim anim second second second second second second second second second second second second second second second second\n",
      "\n",
      "conf conf she she she she she scrape conf she she she she she Dresden Dresden Dresden Dresden Dresden she she she she she conf conf conf conf conf thrust she husband\n",
      "\n",
      "Mit Mit Mit Mit IA Mit Ob Gov Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob Ob IA IA\n",
      "\n",
      "SPONSORED and American Sanford Sm Sanford Sm Sm Black Sm Sm Sm Sm Sm SmSPONSOREDSPONSOREDSPONSORED and and SmSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
      "\n",
      "atheist Clement Ramsey Ramsey union union ethic Thiel Clement Clement Clement Clement Elliott Clement Elliott Elliott Elliott Word Word Word Word Word Pierce Word Word Th ethics ethics Op Th Op Th\n",
      "\n",
      "Amb Amb Amb exclus exclus exclus fid Gr First � exclus sam D sam Amb Amb sam emb emb emb � � � � � � � � � � � �\n",
      "\n",
      "ISI ISI ISI ISI rate rate rate rate rate rate rate rate rate rate em rate � ISI ISI ISI ISI ISI ISI ISI Boko Boko Boko Boko Boko VIDEO VIDEO VIDEO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MyCollator(object):\n",
    "    def __init__(self, encoder_token, decoder_token):\n",
    "        self.encoder_token = encoder_token\n",
    "        self.decoder_token = decoder_token\n",
    "    def __call__(self, batch):\n",
    "        input_ids_bert = pad_sequence([torch.tensor(f['bert_token'], dtype=torch.long) for f in batch],\n",
    "                                  batch_first=True, padding_value=self.encoder_token)\n",
    "        input_ids_gpt = pad_sequence([torch.tensor(f['gpt2_token'], dtype=torch.long) for f in batch],\n",
    "                                    batch_first=True, padding_value=self.decoder_token)\n",
    "        try:\n",
    "            token_lengths = torch.tensor([[len(f['bert_token']), len(f['gpt2_token'])] for f in batch],\n",
    "                                        dtype=torch.long)\n",
    "        except:\n",
    "            token_lengths = torch.zeros((len(batch), 1091))\n",
    "            for i in range(len(batch)):\n",
    "                token_lengths[i, len(batch[i]['gpt2_token'])] = 1\n",
    "        return (input_ids_bert, input_ids_gpt, token_lengths)\n",
    "\n",
    "def main():\n",
    "    encoder_model_class = MODEL_CLASS['BertForLatentConnectorAVG']\n",
    "\n",
    "    #initialize tokenizer and model\n",
    "    print(\"initialize models\")\n",
    "    tokenizer_encoder = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "    latent_size = 64\n",
    "    model_encoder = encoder_model_class.from_pretrained(\"prajjwal1/bert-small\", latent_size=latent_size,\n",
    "                                                        pad_id=tokenizer_encoder.pad_token_id,local_files_only=False)\n",
    "\n",
    "\n",
    "    decoder_model_class = MODEL_CLASS['GPT2ForLatentConnectorNew']\n",
    "    tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "    model_decoder = decoder_model_class.from_pretrained(\"gpt2-xl\", latent_size=latent_size,\n",
    "                                                            latent_as_gpt_emb=True,\n",
    "                                                            latent_as_gpt_memory=True,local_files_only=False)\n",
    "    decoder_n_layer = model_decoder.transformer.config.n_layer\n",
    "    model_decoder.transformer.change_order()\n",
    "\n",
    "    special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>', }\n",
    "    num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "    model_decoder.resize_token_embeddings(len(tokenizer_decoder))\n",
    "    bert_pad_token = tokenizer_encoder.pad_token_id\n",
    "    gpt2_pad_token = tokenizer_decoder.pad_token_id\n",
    "\n",
    "    my_collator = MyCollator(bert_pad_token, gpt2_pad_token)\n",
    "    #download data\n",
    "    print(\"download data\")\n",
    "    train_eval_dataset =load_dataset(\"guangyil/yelp_short_v2\")\n",
    "    eval_dataloader =  DataLoader(train_eval_dataset['test'], num_workers=0, collate_fn=my_collator,batch_size=8)\n",
    "    train_dataloader = DataLoader(train_eval_dataset['train'], num_workers=0, collate_fn=my_collator, batch_size=8)\n",
    "\n",
    "    output_dir = \"test\"\n",
    "    model_vae = VAE(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, latent_size, output_dir)\n",
    "    model_vae.apply(weights_init_rondom)\n",
    "    model_vae.to('cuda:3')   \n",
    "    ddpm = DDPM(eps_model=MLPSkipNet(latent_size), betas=(1e-4, 0.02), n_T=1000, criterion=nn.MSELoss(reduction='none'),).to('cuda:3')   \n",
    "    ddpm.apply(weights_init_rondom)\n",
    "    model = VAE_DDPM(model_vae, ddpm,1.0 ).to('cuda:3')   \n",
    "\n",
    "    calc_ppl_lgy_ddpm(model.model_vae, tokenizer_encoder, tokenizer_decoder, ns=1,\n",
    "                            ddpm=model.ddpm,\n",
    "                            device='cuda:3'\n",
    "                        )\n",
    "\n",
    "print(\"here\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
